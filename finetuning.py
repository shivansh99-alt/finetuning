# -*- coding: utf-8 -*-
"""Copy of Finetuning_Dr_Patient.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5Rs4WRAtivGTw3k4HWU9DTg_hLeghKO
"""

!pip install datasets transformers accelerate tokenizers

from transformers import GPT2Tokenizer, GPT2LMHeadModel,DataCollatorForLanguageModeling,Trainer,TrainingArguments

from datasets import load_dataset

dataset=load_dataset("text",data_files="/content/dr_patient.txt")

dataset



tokenizer=GPT2Tokenizer.from_pretrained("gpt2")

tokenizer.pad_token=tokenizer.eos_token

def tokenize_function(examples):
  tokenizer_inputs=tokenizer(examples["text"],truncation=True,padding="max_length",max_length=128)
  tokenizer_inputs["labels"]=tokenizer_inputs["input_ids"].copy()
  return tokenizer_inputs

tokenized_dataset=dataset.map(tokenize_function,batched=True,num_proc=4,remove_columns=["text"])

print(tokenized_dataset)

model=GPT2LMHeadModel.from_pretrained("gpt2")

model

data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)

training_args=TrainingArguments(
    output_dir="./dr_patient_finetuned",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2
)

trainer=Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"]
)

trainer.train()